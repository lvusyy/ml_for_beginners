{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3d35e8",
   "metadata": {},
   "source": [
    "# 强化学习入门 🎮\n",
    "\n",
    "## 欢迎来到强化学习的世界！\n",
    "\n",
    "想象一下，如果你能让计算机学会\"玩游戏\"和\"做决策\"，是不是很神奇？这就是强化学习的魅力！\n",
    "\n",
    "在这个notebook中，我们将一起探索：\n",
    "- 什么是强化学习？\n",
    "- 为什么要强化学习？\n",
    "- 如何用Python实现简单的强化学习？\n",
    "\n",
    "准备好了吗？让我们开始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d434a",
   "metadata": {},
   "source": [
    "## 1. 什么是强化学习？ 🤔\n",
    "\n",
    "### 生活中的例子\n",
    "想象你在：\n",
    "- 学习骑自行车\n",
    "- 玩电子游戏\n",
    "- 训练宠物\n",
    "\n",
    "这就是强化学习！计算机也可以学会这种技能。\n",
    "\n",
    "### 小测验\n",
    "1. 下面哪些是强化学习的例子？\n",
    "   - [x] 训练机器人走路\n",
    "   - [x] 玩游戏AI\n",
    "   - [ ] 计算1+1=2\n",
    "   - [x] 自动驾驶\n",
    "\n",
    "2. 为什么需要强化学习？\n",
    "   - 帮助我们解决决策问题\n",
    "   - 让计算机学会\"玩游戏\"和\"做决策\"\n",
    "   - 提高工作效率\n",
    "\n",
    "3. 你能想到哪些强化学习的例子？\n",
    "   - 比如：游戏AI、机器人控制、自动驾驶..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f8c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"工具准备完毕！让我们开始吧！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f32f1e",
   "metadata": {},
   "source": [
    "## 2. Q学习：让计算机学会做决策 🎯\n",
    "\n",
    "### 生活中的例子\n",
    "- 选择最佳路径\n",
    "- 选择最佳策略\n",
    "- 选择最佳动作\n",
    "\n",
    "让我们用Python实现一个简单的Q学习算法！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc41994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建简单的环境\n",
    "class SimpleEnv:\n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(2)  # 两个动作：左/右\n",
    "        self.observation_space = spaces.Discrete(4)  # 四个状态\n",
    "        self.state = 0\n",
    "        self.done = False\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        self.done = False\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:  # 向左\n",
    "            self.state = max(0, self.state - 1)\n",
    "        else:  # 向右\n",
    "            self.state = min(3, self.state + 1)\n",
    "            \n",
    "        reward = 1 if self.state == 3 else 0\n",
    "        self.done = self.state == 3\n",
    "        \n",
    "        return self.state, reward, self.done, {}\n",
    "\n",
    "# 创建Q学习算法\n",
    "class QLearning:\n",
    "    def __init__(self, states, actions, learning_rate=0.1, discount_factor=0.9):\n",
    "        self.q_table = np.zeros((states, actions))\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "    def choose_action(self, state, epsilon=0.1):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(2)\n",
    "        return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        old_value = self.q_table[state, action]\n",
    "        next_max = np.max(self.q_table[next_state])\n",
    "        new_value = (1 - self.lr) * old_value + self.lr * (reward + self.gamma * next_max)\n",
    "        self.q_table[state, action] = new_value\n",
    "\n",
    "# 训练智能体\n",
    "env = SimpleEnv()\n",
    "agent = QLearning(4, 2)\n",
    "episodes = 1000\n",
    "rewards = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.learn(state, action, reward, next_state)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# 可视化学习过程\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(rewards)\n",
    "plt.title('Q学习训练过程')\n",
    "plt.xlabel('回合数')\n",
    "plt.ylabel('总奖励')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0554c5",
   "metadata": {},
   "source": [
    "### 动手做一做\n",
    "1. 修改环境参数，观察学习效果如何变化\n",
    "2. 尝试使用不同的学习率\n",
    "3. 观察Q表的变化\n",
    "\n",
    "### 思考题\n",
    "1. 为什么需要Q学习？\n",
    "2. 你能想到其他可以用Q学习解决的问题吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e17a0a",
   "metadata": {},
   "source": [
    "## 3. 策略梯度：让计算机学会优化策略 📈\n",
    "\n",
    "### 生活中的例子\n",
    "- 优化游戏策略\n",
    "- 优化控制策略\n",
    "- 优化决策策略\n",
    "\n",
    "让我们学习如何实现策略梯度算法！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c595950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建策略网络\n",
    "class PolicyNetwork:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        \n",
    "    def forward(self, state):\n",
    "        logits = np.dot(state, self.weights)\n",
    "        probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "        return probs\n",
    "    \n",
    "    def update(self, states, actions, rewards, learning_rate=0.01):\n",
    "        for state, action, reward in zip(states, actions, rewards):\n",
    "            probs = self.forward(state)\n",
    "            gradient = np.zeros_like(self.weights)\n",
    "            gradient[:, action] = state\n",
    "            self.weights += learning_rate * reward * gradient\n",
    "\n",
    "# 训练策略网络\n",
    "env = SimpleEnv()\n",
    "policy = PolicyNetwork(4, 2)\n",
    "episodes = 1000\n",
    "rewards = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    states, actions, rewards_episode = [], [], []\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        state_one_hot = np.zeros(4)\n",
    "        state_one_hot[state] = 1\n",
    "        probs = policy.forward(state_one_hot)\n",
    "        action = np.random.choice(2, p=probs)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        states.append(state_one_hot)\n",
    "        actions.append(action)\n",
    "        rewards_episode.append(reward)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    policy.update(states, actions, rewards_episode)\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# 可视化学习过程\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(rewards)\n",
    "plt.title('策略梯度训练过程')\n",
    "plt.xlabel('回合数')\n",
    "plt.ylabel('总奖励')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d4e2c",
   "metadata": {},
   "source": [
    "### 动手做一做\n",
    "1. 修改网络结构，观察学习效果如何变化\n",
    "2. 尝试使用不同的学习率\n",
    "3. 观察策略的变化\n",
    "\n",
    "### 思考题\n",
    "1. 为什么需要策略梯度？\n",
    "2. 你能想到其他可以用策略梯度解决的问题吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8994169",
   "metadata": {},
   "source": [
    "## 4. 深度强化学习：让计算机学会更复杂的任务 🧠\n",
    "\n",
    "### 生活中的例子\n",
    "- 玩复杂的游戏\n",
    "- 控制复杂的机器人\n",
    "- 解决复杂的问题\n",
    "\n",
    "让我们学习如何实现深度强化学习！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50bd99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建深度Q网络\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.model = models.Sequential([\n",
    "            layers.Dense(24, activation='relu', input_shape=(state_size,)),\n",
    "            layers.Dense(24, activation='relu'),\n",
    "            layers.Dense(action_size, activation='linear')\n",
    "        ])\n",
    "        self.model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "    def choose_action(self, state, epsilon=0.1):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(2)\n",
    "        q_values = self.model.predict(state.reshape(1, -1))\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def train(self, states, actions, rewards, next_states, dones):\n",
    "        targets = self.model.predict(states)\n",
    "        next_q_values = self.model.predict(next_states)\n",
    "        \n",
    "        for i in range(len(states)):\n",
    "            if dones[i]:\n",
    "                targets[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                targets[i][actions[i]] = rewards[i] + 0.9 * np.max(next_q_values[i])\n",
    "                \n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "# 训练深度Q网络\n",
    "env = SimpleEnv()\n",
    "dqn = DQN(4, 2)\n",
    "episodes = 1000\n",
    "rewards = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        state_one_hot = np.zeros(4)\n",
    "        state_one_hot[state] = 1\n",
    "        action = dqn.choose_action(state_one_hot)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state_one_hot = np.zeros(4)\n",
    "        next_state_one_hot[next_state] = 1\n",
    "        \n",
    "        dqn.train(\n",
    "            state_one_hot.reshape(1, -1),\n",
    "            np.array([action]),\n",
    "            np.array([reward]),\n",
    "            next_state_one_hot.reshape(1, -1),\n",
    "            np.array([done])\n",
    "        )\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# 可视化学习过程\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(rewards)\n",
    "plt.title('深度Q学习训练过程')\n",
    "plt.xlabel('回合数')\n",
    "plt.ylabel('总奖励')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3903b",
   "metadata": {},
   "source": [
    "### 动手做一做\n",
    "1. 修改网络结构，观察学习效果如何变化\n",
    "2. 尝试使用不同的优化器\n",
    "3. 观察训练过程\n",
    "\n",
    "### 思考题\n",
    "1. 为什么需要深度强化学习？\n",
    "2. 你能想到其他可以用深度强化学习解决的问题吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f4753",
   "metadata": {},
   "source": [
    "## 5. 趣味练习 🎯\n",
    "\n",
    "### 练习1：简单游戏\n",
    "1. 设计一个简单的游戏环境\n",
    "2. 实现一个强化学习算法\n",
    "3. 训练智能体玩游戏\n",
    "\n",
    "### 练习2：机器人控制\n",
    "1. 设计一个简单的机器人环境\n",
    "2. 实现一个强化学习算法\n",
    "3. 训练机器人完成任务\n",
    "\n",
    "### 练习3：决策优化\n",
    "1. 设计一个简单的决策环境\n",
    "2. 实现一个强化学习算法\n",
    "3. 优化决策策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c672a2",
   "metadata": {},
   "source": [
    "## 6. 知识总结 📝\n",
    "\n",
    "### 今天学到了什么？\n",
    "1. 强化学习的基本概念\n",
    "2. 不同的强化学习算法\n",
    "3. 如何用Python实现强化学习\n",
    "\n",
    "### 下节课预告\n",
    "下次我们将学习如何将前面学到的知识应用到实际项目中！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f456696e",
   "metadata": {},
   "source": [
    "## 7. 趣味问答 🤔\n",
    "\n",
    "1. 选择题\n",
    "   - 下面哪个不是强化学习的例子？\n",
    "     a) 训练机器人走路\n",
    "     b) 计算1+1=2\n",
    "     c) 玩游戏AI\n",
    "     d) 自动驾驶\n",
    "\n",
    "2. 判断题\n",
    "   - 强化学习就是让计算机\"玩游戏\"和\"做决策\"（对/错）\n",
    "   - 奖励对强化学习很重要（对/错）\n",
    "\n",
    "3. 开放题\n",
    "   - 你能想到哪些有趣的强化学习问题？\n",
    "   - 为什么强化学习很重要？\n",
    "   - 你觉得强化学习会如何改变我们的生活？"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
