{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3d35e8",
   "metadata": {},
   "source": [
    "# å¼ºåŒ–å­¦ä¹ å…¥é—¨ ğŸ®\n",
    "\n",
    "## æ¬¢è¿æ¥åˆ°å¼ºåŒ–å­¦ä¹ çš„ä¸–ç•Œï¼\n",
    "\n",
    "æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœä½ èƒ½è®©è®¡ç®—æœºå­¦ä¼š\"ç©æ¸¸æˆ\"å’Œ\"åšå†³ç­–\"ï¼Œæ˜¯ä¸æ˜¯å¾ˆç¥å¥‡ï¼Ÿè¿™å°±æ˜¯å¼ºåŒ–å­¦ä¹ çš„é­…åŠ›ï¼\n",
    "\n",
    "åœ¨è¿™ä¸ªnotebookä¸­ï¼Œæˆ‘ä»¬å°†ä¸€èµ·æ¢ç´¢ï¼š\n",
    "- ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ\n",
    "- ä¸ºä»€ä¹ˆè¦å¼ºåŒ–å­¦ä¹ ï¼Ÿ\n",
    "- å¦‚ä½•ç”¨Pythonå®ç°ç®€å•çš„å¼ºåŒ–å­¦ä¹ ï¼Ÿ\n",
    "\n",
    "å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹å§ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d434a",
   "metadata": {},
   "source": [
    "## 1. ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ ğŸ¤”\n",
    "\n",
    "### ç”Ÿæ´»ä¸­çš„ä¾‹å­\n",
    "æƒ³è±¡ä½ åœ¨ï¼š\n",
    "- å­¦ä¹ éª‘è‡ªè¡Œè½¦\n",
    "- ç©ç”µå­æ¸¸æˆ\n",
    "- è®­ç»ƒå® ç‰©\n",
    "\n",
    "è¿™å°±æ˜¯å¼ºåŒ–å­¦ä¹ ï¼è®¡ç®—æœºä¹Ÿå¯ä»¥å­¦ä¼šè¿™ç§æŠ€èƒ½ã€‚\n",
    "\n",
    "### å°æµ‹éªŒ\n",
    "1. ä¸‹é¢å“ªäº›æ˜¯å¼ºåŒ–å­¦ä¹ çš„ä¾‹å­ï¼Ÿ\n",
    "   - [x] è®­ç»ƒæœºå™¨äººèµ°è·¯\n",
    "   - [x] ç©æ¸¸æˆAI\n",
    "   - [ ] è®¡ç®—1+1=2\n",
    "   - [x] è‡ªåŠ¨é©¾é©¶\n",
    "\n",
    "2. ä¸ºä»€ä¹ˆéœ€è¦å¼ºåŒ–å­¦ä¹ ï¼Ÿ\n",
    "   - å¸®åŠ©æˆ‘ä»¬è§£å†³å†³ç­–é—®é¢˜\n",
    "   - è®©è®¡ç®—æœºå­¦ä¼š\"ç©æ¸¸æˆ\"å’Œ\"åšå†³ç­–\"\n",
    "   - æé«˜å·¥ä½œæ•ˆç‡\n",
    "\n",
    "3. ä½ èƒ½æƒ³åˆ°å“ªäº›å¼ºåŒ–å­¦ä¹ çš„ä¾‹å­ï¼Ÿ\n",
    "   - æ¯”å¦‚ï¼šæ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€è‡ªåŠ¨é©¾é©¶..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f8c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"å·¥å…·å‡†å¤‡å®Œæ¯•ï¼è®©æˆ‘ä»¬å¼€å§‹å§ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f32f1e",
   "metadata": {},
   "source": [
    "## 2. Qå­¦ä¹ ï¼šè®©è®¡ç®—æœºå­¦ä¼šåšå†³ç­– ğŸ¯\n",
    "\n",
    "### ç”Ÿæ´»ä¸­çš„ä¾‹å­\n",
    "- é€‰æ‹©æœ€ä½³è·¯å¾„\n",
    "- é€‰æ‹©æœ€ä½³ç­–ç•¥\n",
    "- é€‰æ‹©æœ€ä½³åŠ¨ä½œ\n",
    "\n",
    "è®©æˆ‘ä»¬ç”¨Pythonå®ç°ä¸€ä¸ªç®€å•çš„Qå­¦ä¹ ç®—æ³•ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc41994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç®€å•çš„ç¯å¢ƒ\n",
    "class SimpleEnv:\n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(2)  # ä¸¤ä¸ªåŠ¨ä½œï¼šå·¦/å³\n",
    "        self.observation_space = spaces.Discrete(4)  # å››ä¸ªçŠ¶æ€\n",
    "        self.state = 0\n",
    "        self.done = False\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        self.done = False\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:  # å‘å·¦\n",
    "            self.state = max(0, self.state - 1)\n",
    "        else:  # å‘å³\n",
    "            self.state = min(3, self.state + 1)\n",
    "            \n",
    "        reward = 1 if self.state == 3 else 0\n",
    "        self.done = self.state == 3\n",
    "        \n",
    "        return self.state, reward, self.done, {}\n",
    "\n",
    "# åˆ›å»ºQå­¦ä¹ ç®—æ³•\n",
    "class QLearning:\n",
    "    def __init__(self, states, actions, learning_rate=0.1, discount_factor=0.9):\n",
    "        self.q_table = np.zeros((states, actions))\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "    def choose_action(self, state, epsilon=0.1):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(2)\n",
    "        return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        old_value = self.q_table[state, action]\n",
    "        next_max = np.max(self.q_table[next_state])\n",
    "        new_value = (1 - self.lr) * old_value + self.lr * (reward + self.gamma * next_max)\n",
    "        self.q_table[state, action] = new_value\n",
    "\n",
    "# è®­ç»ƒæ™ºèƒ½ä½“\n",
    "env = SimpleEnv()\n",
    "agent = QLearning(4, 2)\n",
    "episodes = 1000\n",
    "rewards = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.learn(state, action, reward, next_state)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# å¯è§†åŒ–å­¦ä¹ è¿‡ç¨‹\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(rewards)\n",
    "plt.title('Qå­¦ä¹ è®­ç»ƒè¿‡ç¨‹')\n",
    "plt.xlabel('å›åˆæ•°')\n",
    "plt.ylabel('æ€»å¥–åŠ±')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0554c5",
   "metadata": {},
   "source": [
    "### åŠ¨æ‰‹åšä¸€åš\n",
    "1. ä¿®æ”¹ç¯å¢ƒå‚æ•°ï¼Œè§‚å¯Ÿå­¦ä¹ æ•ˆæœå¦‚ä½•å˜åŒ–\n",
    "2. å°è¯•ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡\n",
    "3. è§‚å¯ŸQè¡¨çš„å˜åŒ–\n",
    "\n",
    "### æ€è€ƒé¢˜\n",
    "1. ä¸ºä»€ä¹ˆéœ€è¦Qå­¦ä¹ ï¼Ÿ\n",
    "2. ä½ èƒ½æƒ³åˆ°å…¶ä»–å¯ä»¥ç”¨Qå­¦ä¹ è§£å†³çš„é—®é¢˜å—ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e17a0a",
   "metadata": {},
   "source": [
    "## 3. ç­–ç•¥æ¢¯åº¦ï¼šè®©è®¡ç®—æœºå­¦ä¼šä¼˜åŒ–ç­–ç•¥ ğŸ“ˆ\n",
    "\n",
    "### ç”Ÿæ´»ä¸­çš„ä¾‹å­\n",
    "- ä¼˜åŒ–æ¸¸æˆç­–ç•¥\n",
    "- ä¼˜åŒ–æ§åˆ¶ç­–ç•¥\n",
    "- ä¼˜åŒ–å†³ç­–ç­–ç•¥\n",
    "\n",
    "è®©æˆ‘ä»¬å­¦ä¹ å¦‚ä½•å®ç°ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c595950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç­–ç•¥ç½‘ç»œ\n",
    "class PolicyNetwork:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        \n",
    "    def forward(self, state):\n",
    "        logits = np.dot(state, self.weights)\n",
    "        probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "        return probs\n",
    "    \n",
    "    def update(self, states, actions, rewards, learning_rate=0.01):\n",
    "        for state, action, reward in zip(states, actions, rewards):\n",
    "            probs = self.forward(state)\n",
    "            gradient = np.zeros_like(self.weights)\n",
    "            gradient[:, action] = state\n",
    "            self.weights += learning_rate * reward * gradient\n",
    "\n",
    "# è®­ç»ƒç­–ç•¥ç½‘ç»œ\n",
    "env = SimpleEnv()\n",
    "policy = PolicyNetwork(4, 2)\n",
    "episodes = 1000\n",
    "rewards = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    states, actions, rewards_episode = [], [], []\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        state_one_hot = np.zeros(4)\n",
    "        state_one_hot[state] = 1\n",
    "        probs = policy.forward(state_one_hot)\n",
    "        action = np.random.choice(2, p=probs)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        states.append(state_one_hot)\n",
    "        actions.append(action)\n",
    "        rewards_episode.append(reward)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    policy.update(states, actions, rewards_episode)\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# å¯è§†åŒ–å­¦ä¹ è¿‡ç¨‹\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(rewards)\n",
    "plt.title('ç­–ç•¥æ¢¯åº¦è®­ç»ƒè¿‡ç¨‹')\n",
    "plt.xlabel('å›åˆæ•°')\n",
    "plt.ylabel('æ€»å¥–åŠ±')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d4e2c",
   "metadata": {},
   "source": [
    "### åŠ¨æ‰‹åšä¸€åš\n",
    "1. ä¿®æ”¹ç½‘ç»œç»“æ„ï¼Œè§‚å¯Ÿå­¦ä¹ æ•ˆæœå¦‚ä½•å˜åŒ–\n",
    "2. å°è¯•ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡\n",
    "3. è§‚å¯Ÿç­–ç•¥çš„å˜åŒ–\n",
    "\n",
    "### æ€è€ƒé¢˜\n",
    "1. ä¸ºä»€ä¹ˆéœ€è¦ç­–ç•¥æ¢¯åº¦ï¼Ÿ\n",
    "2. ä½ èƒ½æƒ³åˆ°å…¶ä»–å¯ä»¥ç”¨ç­–ç•¥æ¢¯åº¦è§£å†³çš„é—®é¢˜å—ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8994169",
   "metadata": {},
   "source": [
    "## 4. æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼šè®©è®¡ç®—æœºå­¦ä¼šæ›´å¤æ‚çš„ä»»åŠ¡ ğŸ§ \n",
    "\n",
    "### ç”Ÿæ´»ä¸­çš„ä¾‹å­\n",
    "- ç©å¤æ‚çš„æ¸¸æˆ\n",
    "- æ§åˆ¶å¤æ‚çš„æœºå™¨äºº\n",
    "- è§£å†³å¤æ‚çš„é—®é¢˜\n",
    "\n",
    "è®©æˆ‘ä»¬å­¦ä¹ å¦‚ä½•å®ç°æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50bd99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ·±åº¦Qç½‘ç»œ\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.model = models.Sequential([\n",
    "            layers.Dense(24, activation='relu', input_shape=(state_size,)),\n",
    "            layers.Dense(24, activation='relu'),\n",
    "            layers.Dense(action_size, activation='linear')\n",
    "        ])\n",
    "        self.model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "    def choose_action(self, state, epsilon=0.1):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(2)\n",
    "        q_values = self.model.predict(state.reshape(1, -1))\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def train(self, states, actions, rewards, next_states, dones):\n",
    "        targets = self.model.predict(states)\n",
    "        next_q_values = self.model.predict(next_states)\n",
    "        \n",
    "        for i in range(len(states)):\n",
    "            if dones[i]:\n",
    "                targets[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                targets[i][actions[i]] = rewards[i] + 0.9 * np.max(next_q_values[i])\n",
    "                \n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "\n",
    "# è®­ç»ƒæ·±åº¦Qç½‘ç»œ\n",
    "env = SimpleEnv()\n",
    "dqn = DQN(4, 2)\n",
    "episodes = 1000\n",
    "rewards = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        state_one_hot = np.zeros(4)\n",
    "        state_one_hot[state] = 1\n",
    "        action = dqn.choose_action(state_one_hot)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state_one_hot = np.zeros(4)\n",
    "        next_state_one_hot[next_state] = 1\n",
    "        \n",
    "        dqn.train(\n",
    "            state_one_hot.reshape(1, -1),\n",
    "            np.array([action]),\n",
    "            np.array([reward]),\n",
    "            next_state_one_hot.reshape(1, -1),\n",
    "            np.array([done])\n",
    "        )\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    rewards.append(total_reward)\n",
    "\n",
    "# å¯è§†åŒ–å­¦ä¹ è¿‡ç¨‹\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(rewards)\n",
    "plt.title('æ·±åº¦Qå­¦ä¹ è®­ç»ƒè¿‡ç¨‹')\n",
    "plt.xlabel('å›åˆæ•°')\n",
    "plt.ylabel('æ€»å¥–åŠ±')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3903b",
   "metadata": {},
   "source": [
    "### åŠ¨æ‰‹åšä¸€åš\n",
    "1. ä¿®æ”¹ç½‘ç»œç»“æ„ï¼Œè§‚å¯Ÿå­¦ä¹ æ•ˆæœå¦‚ä½•å˜åŒ–\n",
    "2. å°è¯•ä½¿ç”¨ä¸åŒçš„ä¼˜åŒ–å™¨\n",
    "3. è§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹\n",
    "\n",
    "### æ€è€ƒé¢˜\n",
    "1. ä¸ºä»€ä¹ˆéœ€è¦æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Ÿ\n",
    "2. ä½ èƒ½æƒ³åˆ°å…¶ä»–å¯ä»¥ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ è§£å†³çš„é—®é¢˜å—ï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f4753",
   "metadata": {},
   "source": [
    "## 5. è¶£å‘³ç»ƒä¹  ğŸ¯\n",
    "\n",
    "### ç»ƒä¹ 1ï¼šç®€å•æ¸¸æˆ\n",
    "1. è®¾è®¡ä¸€ä¸ªç®€å•çš„æ¸¸æˆç¯å¢ƒ\n",
    "2. å®ç°ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•\n",
    "3. è®­ç»ƒæ™ºèƒ½ä½“ç©æ¸¸æˆ\n",
    "\n",
    "### ç»ƒä¹ 2ï¼šæœºå™¨äººæ§åˆ¶\n",
    "1. è®¾è®¡ä¸€ä¸ªç®€å•çš„æœºå™¨äººç¯å¢ƒ\n",
    "2. å®ç°ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•\n",
    "3. è®­ç»ƒæœºå™¨äººå®Œæˆä»»åŠ¡\n",
    "\n",
    "### ç»ƒä¹ 3ï¼šå†³ç­–ä¼˜åŒ–\n",
    "1. è®¾è®¡ä¸€ä¸ªç®€å•çš„å†³ç­–ç¯å¢ƒ\n",
    "2. å®ç°ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•\n",
    "3. ä¼˜åŒ–å†³ç­–ç­–ç•¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c672a2",
   "metadata": {},
   "source": [
    "## 6. çŸ¥è¯†æ€»ç»“ ğŸ“\n",
    "\n",
    "### ä»Šå¤©å­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ\n",
    "1. å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ\n",
    "2. ä¸åŒçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•\n",
    "3. å¦‚ä½•ç”¨Pythonå®ç°å¼ºåŒ–å­¦ä¹ \n",
    "\n",
    "### ä¸‹èŠ‚è¯¾é¢„å‘Š\n",
    "ä¸‹æ¬¡æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•å°†å‰é¢å­¦åˆ°çš„çŸ¥è¯†åº”ç”¨åˆ°å®é™…é¡¹ç›®ä¸­ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f456696e",
   "metadata": {},
   "source": [
    "## 7. è¶£å‘³é—®ç­” ğŸ¤”\n",
    "\n",
    "1. é€‰æ‹©é¢˜\n",
    "   - ä¸‹é¢å“ªä¸ªä¸æ˜¯å¼ºåŒ–å­¦ä¹ çš„ä¾‹å­ï¼Ÿ\n",
    "     a) è®­ç»ƒæœºå™¨äººèµ°è·¯\n",
    "     b) è®¡ç®—1+1=2\n",
    "     c) ç©æ¸¸æˆAI\n",
    "     d) è‡ªåŠ¨é©¾é©¶\n",
    "\n",
    "2. åˆ¤æ–­é¢˜\n",
    "   - å¼ºåŒ–å­¦ä¹ å°±æ˜¯è®©è®¡ç®—æœº\"ç©æ¸¸æˆ\"å’Œ\"åšå†³ç­–\"ï¼ˆå¯¹/é”™ï¼‰\n",
    "   - å¥–åŠ±å¯¹å¼ºåŒ–å­¦ä¹ å¾ˆé‡è¦ï¼ˆå¯¹/é”™ï¼‰\n",
    "\n",
    "3. å¼€æ”¾é¢˜\n",
    "   - ä½ èƒ½æƒ³åˆ°å“ªäº›æœ‰è¶£çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Ÿ\n",
    "   - ä¸ºä»€ä¹ˆå¼ºåŒ–å­¦ä¹ å¾ˆé‡è¦ï¼Ÿ\n",
    "   - ä½ è§‰å¾—å¼ºåŒ–å­¦ä¹ ä¼šå¦‚ä½•æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»ï¼Ÿ"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
